{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tooldev import RST\n",
    "from tooldev.sample import bootstrap\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('datasets/d1.csv') # Orignal Dataset shape (381109, 11)\n",
    "# data = data.drop(['id'], axis=1)\n",
    "\n",
    "data = pd.read_csv('datasets/d1_bootstraped.csv')\n",
    "\n",
    "# data_2 = pd.read_csv('datasets/d2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for creating a new sample of the datasets with bootstrap\n",
    "# freq = pd.DataFrame({'Response':['0', '1'], 'nostoextract':[46710, 46710], })\n",
    "# data = bootstrap(data,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender                    2\n",
       "Driving_License           1\n",
       "Region_Code              34\n",
       "Previously_Insured        2\n",
       "Vehicle_Age               3\n",
       "Vehicle_Damage            2\n",
       "Annual_Premium          170\n",
       "Policy_Sales_Channel     23\n",
       "Response                  2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = data.copy()\n",
    "df = df.sample(200, random_state=41) # 24\n",
    "df['Response'].value_counts().plot(kind='bar')\n",
    "df.drop(['Vintage', 'Age'], axis=1, inplace=True)\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual_Premium    5\n",
      "dtype: int64\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Vehicle_Damage</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Response</th>\n",
       "      <th>Annual_Premium_AFTER</th>\n",
       "      <th>Annual_Premium_STAT_AFTER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35591.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35658.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35805.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36210.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36273.0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50447.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50563.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50818.0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51709.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52453.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gender  Driving_License  Region_Code  Previously_Insured  Vehicle_Age  \\\n",
       "124       1                0           27                   1            0   \n",
       "100       0                0           18                   0            0   \n",
       "34        0                0           18                   0            0   \n",
       "107       0                0           10                   0            0   \n",
       "112       0                0           24                   0            1   \n",
       "..      ...              ...          ...                 ...          ...   \n",
       "98        0                0           18                   0            0   \n",
       "5         1                0           18                   0            0   \n",
       "95        1                0           10                   1            1   \n",
       "176       1                0           27                   0            1   \n",
       "18        0                0           18                   0            0   \n",
       "\n",
       "     Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  Response  \\\n",
       "124               0         35591.0                    13         0   \n",
       "100               1         35658.0                     5         1   \n",
       "34                1         35805.0                    12         0   \n",
       "107               1         36210.0                    20         1   \n",
       "112               0         36273.0                    15         0   \n",
       "..              ...             ...                   ...       ...   \n",
       "98                1         50447.0                     5         1   \n",
       "5                 1         50563.0                    13         1   \n",
       "95                0         50818.0                    15         0   \n",
       "176               1         51709.0                    13         1   \n",
       "18                1         52453.0                    13         1   \n",
       "\n",
       "     Annual_Premium_AFTER  Annual_Premium_STAT_AFTER  \n",
       "124                     0                          0  \n",
       "100                     0                          0  \n",
       "34                      0                          0  \n",
       "107                     0                          0  \n",
       "112                     0                          0  \n",
       "..                    ...                        ...  \n",
       "98                      0                          0  \n",
       "5                       0                          0  \n",
       "95                      0                          0  \n",
       "176                     0                          0  \n",
       "18                      0                          0  \n",
       "\n",
       "[66 rows x 11 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "class Error(Exception):\n",
    "    \"\"\"Base class for other exceptions\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class NotMatchResult(Error):\n",
    "    \"\"\"Raised when the input value is too small\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class DRST:\n",
    "    '''\n",
    "        An Efficient Discretization Algorithm Using Rough Set Theory\n",
    "\n",
    "        ...\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        checker_type : str='topN'\n",
    "            Type of the checker of data to determind wether data column is Continous or Discrete\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        fit(x: DataFrame, continous_columns: list)\n",
    "\n",
    "        Note\n",
    "        -------\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        checker_type: str = 'topN',\n",
    "        comb_max_depth: int = None,\n",
    "        decision_column_name: str = 'class',\n",
    "    ):\n",
    "\n",
    "        # Primary Data Information\n",
    "        self.data = None\n",
    "        self.prepared_data = None\n",
    "        self.columns = None\n",
    "        self.continuous_columns = None\n",
    "        self.discrete_columns = None\n",
    "        self.scaled_data = None\n",
    "        self.decision_column_name = decision_column_name\n",
    "\n",
    "        # Secondary Data infromation\n",
    "        self.data_after_INI = None\n",
    "        self.silhouette_scores = None\n",
    "        self.lower_edge_value = {}\n",
    "        self.upper_edge_value = {}\n",
    "        self.outer_edge_value = {}\n",
    "        \n",
    "        # check_discrete\n",
    "        self.dis_check_threshold = 0.5\n",
    "        self.dis_checker = checker_type  # ratio\n",
    "        self.checker_top_n = 10  # defult number of top N for topN checker\n",
    "\n",
    "        # Config Tools\n",
    "        self.rst = None\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.encoder_dict = dict()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.natural_interval_model = 'kmeans'\n",
    "        self.NIM_message = 'Model for initiat the natural intervals cannot be blank'\n",
    "        self.comb_max_depth = comb_max_depth\n",
    "        self.all_combinations = None\n",
    "\n",
    "    def fit(\n",
    "            self,\n",
    "            x: pd.DataFrame,\n",
    "            continous_columns: list = [],\n",
    "            natural_interval_model: str = 'kmeans'\n",
    "    ):\n",
    "        \"\"\"Discretization using Rough Sets Theory (RST).\n",
    "\n",
    "        Fit `x` to an efficient intervals using the concept of RST \n",
    "        * asd\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : DataFrame\n",
    "            Full DataFrame (default is None)\n",
    "\n",
    "        continous_columns : list\n",
    "            Determind names of column(s) in the DataFarme as Continuous data. If `None` a function _check_continuous triger to determind names of column(s).\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        NotMatchResult\n",
    "            If continous_columns `~None` model check the input with the checker determation if didn't match the result\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Reduct Data Intervals phase: Pre-process of discretization, intiate classification\n",
    "        # Pre-process of discretization\n",
    "        self.data = x\n",
    "        self.columns = self.data.columns\n",
    "        self.continuous_columns = continous_columns if continous_columns else self._check_continuous()\n",
    "        self.discrete_columns = list(set(self.columns).difference(self.continuous_columns))\n",
    "        self.discrete_columns.remove(self.decision_column_name)\n",
    "        self.scaled_data = self._scaling_continuous()\n",
    "        self.silhouette_scores = self._get_silhouette_score()\n",
    "\n",
    "        # Intiate classification\n",
    "        self.data_after_INI = getattr(self, '_%s_model' % natural_interval_model, lambda: self.NIM_message)()\n",
    "        print(self.data_after_INI.nunique())\n",
    "        self.data_after_INI.sort_values(['Annual_Premium']).to_csv('%s_model.csv' % natural_interval_model)\n",
    "        self.all_combinations = self.get_combinations()\n",
    "\n",
    "        data = self.data.copy()  # local datavriable to save change in\n",
    "        # data[self.continuous_columns] = self.data_after_INI.to_numpy()\n",
    "        data = self._labeling_discrete(data)\n",
    "        self.data_after_INI[[(i + '_STAT') for i in self.data_after_INI.columns]] = self.data_after_INI[self.data_after_INI.columns]\n",
    "        self.data_after_INI.columns = map((lambda x: x + '_AFTER'), self.data_after_INI.columns)\n",
    "\n",
    "        data = pd.concat([\n",
    "            data.reset_index(drop=True),\n",
    "            self.data_after_INI.reset_index(drop=True)\n",
    "        ], axis=1)\n",
    "\n",
    "        dep = {}\n",
    "        org_dep = {}\n",
    "\n",
    "        # Duplicate Continuous columns\n",
    "        for continuous_att in self.continuous_columns:\n",
    "            self.prepared_data = data.copy()\n",
    "            self.prepared_data = data.sort_values([continuous_att + '_AFTER', continuous_att])\n",
    "            label_simulator = self.silhouette_scores[continuous_att]\n",
    "            for combination in self.all_combinations:\n",
    "                _combination = list(combination)\n",
    "                _combination.append(continuous_att + '_AFTER')\n",
    "                self.rst = RST(\n",
    "                    self.prepared_data,\n",
    "                    continuous_columns=self.continuous_columns,\n",
    "                    decision_column_name=self.decision_column_name,\n",
    "                    include_continuous_col=True\n",
    "                )\n",
    "                lower, dependency = self.rst.get_dependency(\n",
    "                    combination=list(_combination)\n",
    "                )\n",
    "                \n",
    "                org_dep.update({\n",
    "                        '_'.join(i for i in _combination): dependency\n",
    "                    })\n",
    "                \n",
    "                for value in range(0, label_simulator):\n",
    "                    self.lower_data = data.copy()\n",
    "                    self.upper_data = data.copy()\n",
    "                    self.both_data = data.copy()\n",
    "                    indices = self.prepared_data[self.prepared_data[continuous_att + '_AFTER'] == value].index.tolist()\n",
    "                    if value not in dep.keys():\n",
    "                        dep.update({value: {\n",
    "                            'lower':[], \n",
    "                            'upper':[]\n",
    "                            }})\n",
    "\n",
    "                    self._set_lower_edge_value(continuous_att, indices, value, label_simulator)\n",
    "                    self._set_upper_edge_value(continuous_att, indices, value, label_simulator)\n",
    "                    self._set_outer_edge_values(continuous_att, indices, value)\n",
    "                    # if len(indices) > 1:\n",
    "                    if value not in [0]:\n",
    "                        self.lower_data.loc[self.lower_edge_value[value]['idx'], (continuous_att + '_AFTER')] = value + 1\n",
    "                        # self.both_data.loc[self.lower_edge_value[value]['idx'], (continuous_att + '_AFTER')] = value + 1\n",
    "                        self.rst = RST(\n",
    "                            self.lower_data,\n",
    "                            continuous_columns=self.continuous_columns,\n",
    "                            decision_column_name=self.decision_column_name,\n",
    "                            include_continuous_col=True\n",
    "                        )\n",
    "                        lower, dependency = self.rst.get_dependency(\n",
    "                            combination=list(_combination)\n",
    "                        )\n",
    "                        dep[value]['lower'].append(dependency)\n",
    "\n",
    "                \n",
    "                    if value not in [(label_simulator - 1)]:\n",
    "                        self.upper_data.loc[self.upper_edge_value[value]['idx'], (continuous_att + '_AFTER')] = value - 1\n",
    "                        # self.both_data.loc[self.upper_edge_value[value]['idx'], (continuous_att + '_AFTER')] = value - 1\n",
    "                        self.rst = RST(\n",
    "                            self.upper_data,\n",
    "                            continuous_columns=self.continuous_columns,\n",
    "                            decision_column_name=self.decision_column_name,\n",
    "                            include_continuous_col=True\n",
    "                        )\n",
    "                        lower, dependency = self.rst.get_dependency(\n",
    "                            combination=list(_combination)\n",
    "                        )\n",
    "                        dep[value]['upper'].append(dependency)\n",
    "                    # elif\n",
    "                    # self.rst = RST(\n",
    "                    #     self.both_data,\n",
    "                    #     continuous_columns=self.continuous_columns,\n",
    "                    #     decision_column_name=self.decision_column_name,\n",
    "                    #     include_continuous_col=True\n",
    "                    # )\n",
    "                    # lower, dependency = self.rst.get_dependency(\n",
    "                    #     combination=list(_combination)\n",
    "                    # )\n",
    "                    # dep[value]['both'].append(dependency)\n",
    "\n",
    "                    value += 1\n",
    "        dep = collections.OrderedDict(sorted(dep.items()))       \n",
    "        with open('dep.json', 'w+') as fp:\n",
    "            json.dump(dep, fp)\n",
    "        # print(self.lower_edge_value)\n",
    "        # print('--------------------------------')\n",
    "        # print(self.upper_edge_value)\n",
    "        # print('--------------------------------')\n",
    "        # print(self.outer_edge_value)\n",
    "        return self.prepared_data\n",
    "\n",
    "    def _dbscan_model(self):\n",
    "        result = {}\n",
    "        for att in self.continuous_columns:\n",
    "            clustering = DBSCAN(eps=1, min_samples=5).fit(\n",
    "                self._np_array_reshaped(self.data[att])\n",
    "            )\n",
    "            result[att] = clustering.labels_\n",
    "        return pd.DataFrame(result)\n",
    "\n",
    "    def _kmeans_model(self):\n",
    "        result = {}\n",
    "        for att in self.continuous_columns:\n",
    "            clustering = KMeans(n_clusters=self.silhouette_scores[att], random_state=41).fit(\n",
    "                self._np_array_reshaped(self.scaled_data[att])\n",
    "            )\n",
    "            result[att] = clustering.labels_\n",
    "        return pd.DataFrame(result)\n",
    "\n",
    "    def _np_array_reshaped(self, data, reshape=(-1, 1)):\n",
    "        return np.array(data.tolist()).reshape(reshape[0], reshape[1])\n",
    "\n",
    "    def _check_continuous(self, continous_columns: list = []):\n",
    "        top_n = self.checker_top_n\n",
    "        likely = []\n",
    "\n",
    "        # check if attribute is continuous or discrete dataframe\n",
    "        for var in self.data.columns:\n",
    "            if self.dis_checker == 'topN':\n",
    "                # Check if the top n unique values account for more than a certain proportion of all values\n",
    "                if 1.*self.data[var].value_counts(normalize=True).head(top_n).sum() < 0.5:\n",
    "                    likely.append(var)\n",
    "            elif self.dis_checker == 'ratio':\n",
    "                # Find the ratio of number of unique values to the total number of unique values. Something like the following\n",
    "                if 1.*self.data[var].nunique()/self.data[var].count() > 0.5:\n",
    "                    likely.append(var)\n",
    "\n",
    "        common_names = ['id']\n",
    "\n",
    "        return [x for x in likely if x.lower() not in common_names]\n",
    "\n",
    "    def _labeling_discrete(self, data):\n",
    "\n",
    "        data_endocded = data.copy()\n",
    "\n",
    "        for col in data_endocded:\n",
    "            if col not in self.continuous_columns:\n",
    "                le = self.encoder.fit(data_endocded[col])\n",
    "                data_endocded[col] = self.encoder.transform(data_endocded[col])\n",
    "                self.encoder_dict[col] = self.encoder\n",
    "        return data_endocded\n",
    "\n",
    "    def _scaling_continuous(self):\n",
    "\n",
    "        data_cluster = self.data[self.continuous_columns].copy()\n",
    "        scaled_columns = self.scaler.fit_transform(data_cluster)\n",
    "        self.scaled_data = self.data.copy()\n",
    "        self.scaled_data[self.continuous_columns] = scaled_columns\n",
    "\n",
    "        return self.scaled_data\n",
    "\n",
    "    def _get_silhouette_score(self):\n",
    "        scores = {}\n",
    "        for att in self.continuous_columns:\n",
    "            temp_score = []\n",
    "            scores[att] = 3\n",
    "            for n_clusters in range(3, 9):\n",
    "                # Initialize the clusterer with n_clusters value and a random generator\n",
    "                # seed of 10 for reproducibility.\n",
    "                clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "                cluster_labels = clusterer.fit_predict(\n",
    "                    self._np_array_reshaped(self.scaled_data[att]))\n",
    "\n",
    "                # The silhouette_score gives the average value for all the samples.\n",
    "                # This gives a perspective into the density and separation of the formed\n",
    "                # clusters\n",
    "                silhouette_avg = silhouette_score(\n",
    "                    self._np_array_reshaped(self.scaled_data[att]), cluster_labels)\n",
    "                temp_score.append(silhouette_avg)\n",
    "            scores[att] = temp_score.index(max(temp_score)) + 3\n",
    "        return scores\n",
    "\n",
    "    def get_combinations(self, comb_max_depth: int = None):\n",
    "        if not comb_max_depth:\n",
    "            comb_max_depth = self.comb_max_depth\n",
    "        combX = []\n",
    "        for idx, _ in enumerate(self.discrete_columns):\n",
    "            if comb_max_depth and comb_max_depth == idx:\n",
    "                break\n",
    "            combX.extend(list(combinations(self.discrete_columns, idx + 1)))\n",
    "\n",
    "        return sorted(combX, key=len)\n",
    "    \n",
    "    def _set_lower_edge_value(self, att, indices, value, s_c):\n",
    "        if len(indices) > 1:\n",
    "            same_value = self.prepared_data.loc[indices[::len(indices)-1]][att].values.tolist()\n",
    "        else:\n",
    "            same_value = self.prepared_data.loc[indices][att].values.tolist()\n",
    "\n",
    "        if value not in [0]:\n",
    "            lower_same_value = self.prepared_data[att].isin([same_value[0]])\n",
    "            lower_temp = self.prepared_data[lower_same_value]\n",
    "            self.lower_edge_value.update({\n",
    "                value: {\n",
    "                    'idx': lower_temp.index.tolist(),\n",
    "                    'value': lower_temp.values.tolist()[0][0],\n",
    "                }\n",
    "            })\n",
    "\n",
    "        return self.lower_edge_value\n",
    "\n",
    "    def _set_upper_edge_value(self, att, indices, value, s_c):\n",
    "        if len(indices) > 1:\n",
    "            same_value = self.prepared_data.loc[indices][att].values.tolist()\n",
    "            if value in [s_c]:\n",
    "                upper_same_value = self.prepared_data[att].isin([same_value[1]])\n",
    "            else:\n",
    "                upper_same_value = self.prepared_data[att].isin([same_value[1]])\n",
    "\n",
    "            upper_temp = self.prepared_data[upper_same_value]\n",
    "            self.upper_edge_value.update({\n",
    "                value: {\n",
    "                    'idx': upper_temp.index.tolist(),\n",
    "                    'value': upper_temp.values.tolist()[0][0],\n",
    "                }\n",
    "            })\n",
    "\n",
    "        return self.upper_edge_value\n",
    "\n",
    "    def _set_outer_edge_values(self, att, indices, value):\n",
    "        if len(indices) == 1:\n",
    "            same_value = self.prepared_data.loc[indices][att].values.tolist()\n",
    "            out_same_value = self.prepared_data[att].isin(same_value)\n",
    "            out_temp = self.prepared_data[out_same_value]\n",
    "            self.outer_edge_value.update({\n",
    "                value: {\n",
    "                    'idx': out_temp.index.tolist(),\n",
    "                    'value': out_temp.values.tolist()[0][0],\n",
    "                }\n",
    "            })\n",
    "\n",
    "        return self.outer_edge_value\n",
    "\n",
    "\n",
    "drst = DRST(comb_max_depth=2, decision_column_name='Response')\n",
    "# drst.fit(data_1, natural_interval_model='kmeans', continous_columns=['months_as_customer','total_claim_amount'])\n",
    "# drst.fit(data_1, natural_interval_model='kmeans')\n",
    "\n",
    "drst_fit = drst.fit(df)\n",
    "drst_fit[drst_fit['Annual_Premium_AFTER'] == 0]\n",
    "# drst_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Vehicle_Damage</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Response</th>\n",
       "      <th>Annual_Premium_AFTER</th>\n",
       "      <th>Annual_Premium_STAT_AFTER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>127772.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gender  Driving_License  Region_Code  Previously_Insured  Vehicle_Age  \\\n",
       "31       1                0           18                   0            0   \n",
       "\n",
       "    Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  Response  \\\n",
       "31               1        127772.0                     5         1   \n",
       "\n",
       "    Annual_Premium_AFTER  Annual_Premium_STAT_AFTER  \n",
       "31                     4                          4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drst_fit[drst_fit['Annual_Premium_AFTER'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c34a26dce7bb7449e42c35f5e9b82d8f7c9f77a71ba73bf71e175f0017805aa"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
