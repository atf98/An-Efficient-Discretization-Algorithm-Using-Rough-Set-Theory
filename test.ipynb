{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tooldev import RST\n",
    "from tooldev.sample import bootstrap\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Error(Exception):\n",
    "    \"\"\"Base class for other exceptions\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class NotMatchResult(Error):\n",
    "    \"\"\"Raised when the input value is too small\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class DRST:\n",
    "    '''\n",
    "        An Efficient Discretization Algorithm Using Rough Set Theory\n",
    "\n",
    "        ...\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        checker_type : str='topN'\n",
    "            Type of the checker of data to determind wether data column is Continous or Discrete\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        fit(x: DataFrame, continous_columns: list)\n",
    "\n",
    "        Note\n",
    "        -------\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        checker_type: str = 'topN',\n",
    "        comb_max_depth: int = None,\n",
    "        decision_column_name: str = 'class',\n",
    "        output_loction=None,\n",
    "        save_output=True\n",
    "    ):\n",
    "\n",
    "        # Primary Data Information\n",
    "        self.data = None\n",
    "        self.prepared_data = None\n",
    "        self.columns = None\n",
    "        self.continuous_columns = None\n",
    "        self.discrete_columns = None\n",
    "        self.scaled_data = None\n",
    "        self.decision_column_name = decision_column_name\n",
    "\n",
    "        # Secondary Data infromation\n",
    "        self.data_after_INI = None\n",
    "        self.silhouette_scores = None\n",
    "        self.lower_edge_value = {}\n",
    "        self.upper_edge_value = {}\n",
    "        self.outer_edge_value = {}\n",
    "        \n",
    "        # check_discrete\n",
    "        self.dis_check_threshold = 0.5\n",
    "        self.dis_checker = checker_type  # ratio\n",
    "        self.checker_top_n = 10  # defult number of top N for topN checker\n",
    "\n",
    "        # Config Tools\n",
    "        self.rst = None\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.encoder_dict = dict()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.natural_interval_model = 'kmeans'\n",
    "        self.NIM_message = 'Model for initiat the natural intervals cannot be blank'\n",
    "        self.comb_max_depth = comb_max_depth\n",
    "        self.all_combinations = None\n",
    "        \n",
    "        self.output_loction = os.getcwd() if not output_loction else os.getcwd() + '\\\\' + output_loction\n",
    "        self.save_output = save_output\n",
    "\n",
    "    def fit(\n",
    "            self,\n",
    "            x: pd.DataFrame,\n",
    "            continous_columns: list = [],\n",
    "            natural_interval_model: str = 'kmeans',\n",
    "            ensamble_threshold: int = 0.9\n",
    "    ):\n",
    "        \"\"\"Discretization using Rough Sets Theory (RST).\n",
    "\n",
    "        Fit `x` to an efficient intervals using the concept of RST \n",
    "        * asd\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : DataFrame\n",
    "            Full DataFrame (default is None)\n",
    "\n",
    "        continous_columns : list\n",
    "            Determind names of column(s) in the DataFarme as Continuous data. If `None` a function _check_continuous triger to determind names of column(s).\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        NotMatchResult\n",
    "            If continous_columns `~None` model check the input with the checker determation if didn't match the result\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Reduct Data Intervals phase: Pre-process of discretization, intiate classification\n",
    "        # Pre-process of discretization\n",
    "        self.ensamble_theshold = ensamble_threshold\n",
    "        self.data = x\n",
    "        self.columns = self.data.columns\n",
    "        self.continuous_columns = continous_columns if continous_columns else self._check_continuous()\n",
    "        self.discrete_columns = list(set(self.columns).difference(self.continuous_columns))\n",
    "        self.discrete_columns.remove(self.decision_column_name)\n",
    "        self.scaled_data = self._scaling_continuous()\n",
    "        self.silhouette_scores = self._get_silhouette_score()\n",
    "\n",
    "        # Intiate classification\n",
    "        self.data_after_INI = getattr(self, '_%s_model' % natural_interval_model, lambda: self.NIM_message)()\n",
    "        self.all_combinations = self.get_combinations()\n",
    "        print(self.data_after_INI.nunique())\n",
    "\n",
    "        data = self.data.copy()  # local data vriable to save change in\n",
    "        data = self._labeling_discrete(data)\n",
    "        self.data_after_INI.columns = map((lambda x: x + '_AFTER'), self.data_after_INI.columns)\n",
    "\n",
    "        data = pd.concat([\n",
    "            data.reset_index(drop=True),\n",
    "            self.data_after_INI.reset_index(drop=True)\n",
    "        ], axis=1)\n",
    "\n",
    "        dependency_original_dict = {}\n",
    "        dependency_dict = {}\n",
    "        \n",
    "        # Duplicate Continuous columns\n",
    "        for continuous_att in self.continuous_columns:\n",
    "            att_AFTER = continuous_att + '_AFTER'\n",
    "            label_simulator = self.silhouette_scores[continuous_att]\n",
    "\n",
    "            self.prepared_data = data.copy()\n",
    "            self.prepared_data.sort_values([continuous_att], inplace=True, ignore_index=True)\n",
    "            temp_data = self.prepared_data.copy()\n",
    "            for n_label, o_label in enumerate(temp_data[att_AFTER].unique()):\n",
    "                indices_change_label = temp_data[temp_data[att_AFTER] == o_label].index.tolist()\n",
    "                self.prepared_data.loc[indices_change_label, (att_AFTER)] = n_label\n",
    "            self.prepared_data.sort_values([att_AFTER, continuous_att], inplace=True)\n",
    "\n",
    "            \n",
    "\n",
    "            value = 0\n",
    "            upper_flag = True\n",
    "            lower_flag = True\n",
    "            value_flag = True\n",
    "            value_sufx = ''\n",
    "\n",
    "            while value < label_simulator:\n",
    "                if value_flag:\n",
    "                    upper_flag = True\n",
    "                    lower_flag = True\n",
    "                    value_sufx = ''\n",
    "                indices = self.prepared_data[self.prepared_data[att_AFTER] == value].index.tolist()\n",
    "                if not len(indices) == 0:\n",
    "                    value_dict = '%s%s' % (value, value_sufx)\n",
    "                    if value not in dependency_dict.keys():\n",
    "                        dependency_dict.update({value_dict: {}})\n",
    "                        if upper_flag:\n",
    "                            dependency_dict[value_dict].update({\n",
    "                                'upper': {True: 0, False: 0}\n",
    "                            })\n",
    "\n",
    "                        if lower_flag:\n",
    "                            dependency_dict[value_dict].update({\n",
    "                                'lower': {True: 0, False: 0}\n",
    "                            })\n",
    "\n",
    "                    for combination in self.all_combinations:\n",
    "                        _combination = '|'.join(i for i in combination)\n",
    "                        _combination_AFTER = list(list(combination) + [att_AFTER])\n",
    "\n",
    "                        self.lower_data = self.prepared_data.copy()\n",
    "                        self.upper_data = self.prepared_data.copy()\n",
    "                        \n",
    "                        self.rst = RST(\n",
    "                            self.prepared_data,\n",
    "                            continuous_columns=self.continuous_columns,\n",
    "                            decision_column_name=self.decision_column_name,\n",
    "                            include_continuous_col=True\n",
    "                        )\n",
    "                        _, dependency = self.rst.get_dependency(combination=list(_combination_AFTER))\n",
    "                        dependency_original_dict.update({_combination: dependency})\n",
    "                        \n",
    "                        \n",
    "                        self._set_lower_edge_value(continuous_att, indices, value, label_simulator)\n",
    "                        self._set_upper_edge_value(continuous_att, indices, value, label_simulator)\n",
    "                        \n",
    "                        if value not in [0] and lower_flag:\n",
    "                            self.lower_data.loc[self.lower_edge_value[value]['idx'], (att_AFTER)] = value - 1\n",
    "                            self.rst = RST(\n",
    "                                self.lower_data,\n",
    "                                continuous_columns=self.continuous_columns,\n",
    "                                decision_column_name=self.decision_column_name,\n",
    "                                include_continuous_col=True\n",
    "                            )\n",
    "                            _, dependency = self.rst.get_dependency(\n",
    "                                combination=list(_combination_AFTER)\n",
    "                            )\n",
    "                            dependency_dict[value_dict]['lower'][dependency >= dependency_original_dict[_combination]] += 1\n",
    "                    \n",
    "                        if value not in [label_simulator - 1] and upper_flag:\n",
    "                            self.upper_data.loc[self.upper_edge_value[value]['idx'], (att_AFTER)] = value + 1\n",
    "                            self.rst = RST(\n",
    "                                self.upper_data,\n",
    "                                continuous_columns=self.continuous_columns,\n",
    "                                decision_column_name=self.decision_column_name,\n",
    "                                include_continuous_col=True\n",
    "                            )\n",
    "                            lower, dependency = self.rst.get_dependency(\n",
    "                                combination=list(_combination_AFTER)\n",
    "                            )\n",
    "                            dependency_dict[value_dict]['upper'][dependency >= dependency_original_dict[_combination]] += 1\n",
    "\n",
    "                    if lower_flag:\n",
    "                        is_lower = len(self.all_combinations)*self.ensamble_theshold <= dependency_dict[value_dict]['lower'][True]\n",
    "                    if upper_flag:\n",
    "                        is_upper = len(self.all_combinations)*self.ensamble_theshold <= dependency_dict[value_dict]['upper'][True]\n",
    "\n",
    "                    if is_lower or is_upper:\n",
    "                        lower_flag=is_lower\n",
    "                        upper_flag=is_upper\n",
    "                        value_flag=False\n",
    "                        value_sufx += '-'\n",
    "                        if is_lower:\n",
    "                            self.prepared_data.loc[self.lower_edge_value[value]['idx'], (att_AFTER)] = value - 1\n",
    "                            value_sufx += 'l'\n",
    "                        if is_upper:\n",
    "                            self.prepared_data.loc[self.upper_edge_value[value]['idx'], (att_AFTER)] = value + 1\n",
    "                            value_sufx += 'u'\n",
    "                    else:\n",
    "                        value += 1\n",
    "                        value_flag = True\n",
    "                else:\n",
    "                    value += 1\n",
    "                    value_flag = True\n",
    "\n",
    "        dependency_dict = collections.OrderedDict(sorted(dependency_dict.items()))\n",
    "        if self.save_output:\n",
    "            Path(self.output_loction + '\\\\_log').mkdir(parents=True, exist_ok=True)\n",
    "            with open(self.output_loction + '\\\\_log\\\\logMap.json', 'w+') as fp:\n",
    "                json.dump(dependency_dict, fp)\n",
    "                \n",
    "            self.prepared_data.to_csv(self.output_loction + '\\\\DataAfterDRST.csv')\n",
    "            data.to_csv(self.output_loction + '\\\\DataBeforeDRST.csv')\n",
    "        return [self.prepared_data, self.continuous_columns]\n",
    "\n",
    "    def _dbscan_model(self):\n",
    "        result = {}\n",
    "        for att in self.continuous_columns:\n",
    "            clustering = DBSCAN(eps=1, min_samples=5).fit(\n",
    "                self._np_array_reshaped(self.data[att])\n",
    "            )\n",
    "            result[att] = clustering.labels_\n",
    "        return pd.DataFrame(result)\n",
    "\n",
    "    def _kmeans_model(self):\n",
    "        result = {}\n",
    "        for att in self.continuous_columns:\n",
    "            clustering = KMeans(n_clusters=self.silhouette_scores[att], random_state=41).fit(\n",
    "                self._np_array_reshaped(self.scaled_data[att])\n",
    "            )\n",
    "            result[att] = clustering.labels_\n",
    "        return pd.DataFrame(result)\n",
    "\n",
    "    def _np_array_reshaped(self, data, reshape=(-1, 1)):\n",
    "        return np.array(data.tolist()).reshape(reshape[0], reshape[1])\n",
    "\n",
    "    def _check_continuous(self, continous_columns: list = []):\n",
    "        top_n = self.checker_top_n\n",
    "        likely = []\n",
    "\n",
    "        # check if attribute is continuous or discrete dataframe\n",
    "        for var in self.data.columns:\n",
    "            if self.dis_checker == 'topN':\n",
    "                # Check if the top n unique values account for more than a certain proportion of all values\n",
    "                if 1.*self.data[var].value_counts(normalize=True).head(top_n).sum() < 0.5:\n",
    "                    likely.append(var)\n",
    "            elif self.dis_checker == 'ratio':\n",
    "                # Find the ratio of number of unique values to the total number of unique values. Something like the following\n",
    "                if 1.*self.data[var].nunique()/self.data[var].count() > 0.5:\n",
    "                    likely.append(var)\n",
    "\n",
    "        common_names = ['id']\n",
    "\n",
    "        return [x for x in likely if x.lower() not in common_names]\n",
    "\n",
    "    def _labeling_discrete(self, data):\n",
    "\n",
    "        data_endocded = data.copy()\n",
    "\n",
    "        for col in data_endocded:\n",
    "            if col not in self.continuous_columns:\n",
    "                le = self.encoder.fit(data_endocded[col])\n",
    "                data_endocded[col] = self.encoder.transform(data_endocded[col])\n",
    "                self.encoder_dict[col] = self.encoder\n",
    "        return data_endocded\n",
    "\n",
    "    def _scaling_continuous(self):\n",
    "\n",
    "        data_cluster = self.data[self.continuous_columns].copy()\n",
    "        scaled_columns = self.scaler.fit_transform(data_cluster)\n",
    "        self.scaled_data = self.data.copy()\n",
    "        self.scaled_data[self.continuous_columns] = scaled_columns\n",
    "\n",
    "        return self.scaled_data\n",
    "\n",
    "    def _get_silhouette_score(self):\n",
    "        scores = {}\n",
    "        for att in self.continuous_columns:\n",
    "            temp_score = []\n",
    "            scores[att] = 3\n",
    "            for n_clusters in range(3, 9):\n",
    "                # Initialize the clusterer with n_clusters value and a random generator\n",
    "                # seed of 10 for reproducibility.\n",
    "                clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "                cluster_labels = clusterer.fit_predict(\n",
    "                    self._np_array_reshaped(self.scaled_data[att]))\n",
    "\n",
    "                # The silhouette_score gives the average value for all the samples.\n",
    "                # This gives a perspective into the density and separation of the formed\n",
    "                # clusters\n",
    "                silhouette_avg = silhouette_score(\n",
    "                    self._np_array_reshaped(self.scaled_data[att]), cluster_labels)\n",
    "                temp_score.append(silhouette_avg)\n",
    "            scores[att] = temp_score.index(max(temp_score)) + 3\n",
    "        return scores\n",
    "\n",
    "    def get_combinations(self, comb_max_depth: int = None):\n",
    "        if not comb_max_depth:\n",
    "            comb_max_depth = self.comb_max_depth\n",
    "        combX = []\n",
    "        for idx, _ in enumerate(self.discrete_columns):\n",
    "            if comb_max_depth and comb_max_depth == idx:\n",
    "                break\n",
    "            combX.extend(list(combinations(self.discrete_columns, idx + 1)))\n",
    "\n",
    "        return sorted(combX, key=len)\n",
    "    \n",
    "    def _set_lower_edge_value(self, att, indices, value, s_c):\n",
    "        same_value = self.prepared_data.loc[indices][att].values.tolist()\n",
    "        lower_same_value = self.prepared_data[att].isin([same_value[0]])\n",
    "        lower_temp = self.prepared_data[lower_same_value]\n",
    "        self.lower_edge_value.update({\n",
    "            value: {\n",
    "                'idx': lower_temp.index.tolist(),\n",
    "                'value': lower_temp.values.tolist()[0][0],\n",
    "            }\n",
    "        })\n",
    "\n",
    "        return self.lower_edge_value\n",
    "\n",
    "    def _set_upper_edge_value(self, att, indices, value, s_c):\n",
    "        same_value = self.prepared_data.loc[indices][att].values.tolist()\n",
    "        upper_same_value = self.prepared_data[att].isin([same_value[-1]])\n",
    "        upper_temp = self.prepared_data[upper_same_value]\n",
    "        self.upper_edge_value.update({\n",
    "            value: {\n",
    "                'idx': upper_temp.index.tolist(),\n",
    "                'value': upper_temp.values.tolist()[0][0],\n",
    "            }\n",
    "        })\n",
    "\n",
    "        return self.upper_edge_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature    6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('datasets/play_tennis_three.csv')\n",
    "\n",
    "\n",
    "drst = DRST(comb_max_depth=1, decision_column_name='class', save_output=False)\n",
    "drst_fit, continuous_columns = drst.fit(data, ensamble_threshold=0.1, continous_columns=['temperature'])\n",
    "drst_fit.drop(continuous_columns, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual_Premium    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('datasets/health_insurance.csv')\n",
    "df = data.copy()\n",
    "df = df.sample(2000, random_state=41) # 41\n",
    "df.drop(['Vintage', 'Age'], axis=1, inplace=True)\n",
    "drst = DRST(comb_max_depth=1, decision_column_name='Response', save_output=False)\n",
    "drst_fit, continuous_columns = drst.fit(df, continous_columns=['Annual_Premium'])\n",
    "drst_fit.drop(continuous_columns, axis=1, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e99f971b6d48a9f8afab1d34758ddbb9a87e8265f2671e414e3fdd21b2f0ea4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
