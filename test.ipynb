{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tooldev import RST\n",
    "from tooldev.sample import bootstrap\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Error(Exception):\n",
    "    \"\"\"Base class for other exceptions\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class NotMatchResult(Error):\n",
    "    \"\"\"Raised when the input value is too small\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class DRST:\n",
    "    '''\n",
    "        An Efficient Discretization Algorithm Using Rough Set Theory\n",
    "\n",
    "        ...\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        checker_type : str='topN'\n",
    "            Type of the checker of data to determind wether data column is Continous or Discrete\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        fit(x: DataFrame, continous_columns: list)\n",
    "\n",
    "        Note\n",
    "        -------\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        checker_type: str = 'topN',\n",
    "        comb_max_depth: int = None,\n",
    "        decision_column_name: str = 'class',\n",
    "        output_loction=None,\n",
    "        save_output=True\n",
    "    ):\n",
    "\n",
    "        # Primary Data Information\n",
    "        self.data = None\n",
    "        self.prepared_data = None\n",
    "        self.columns = None\n",
    "        self.continuous_columns = None\n",
    "        self.discrete_columns = None\n",
    "        self.scaled_data = None\n",
    "        self.decision_column_name = decision_column_name\n",
    "\n",
    "        # Secondary Data infromation\n",
    "        self.data_after_INI = None\n",
    "        self.silhouette_scores = None\n",
    "        self.lower_edge_value = {}\n",
    "        self.upper_edge_value = {}\n",
    "        self.outer_edge_value = {}\n",
    "        \n",
    "        # check_discrete\n",
    "        self.dis_check_threshold = 0.5\n",
    "        self.dis_checker = checker_type  # ratio\n",
    "        self.checker_top_n = 10  # defult number of top N for topN checker\n",
    "\n",
    "        # Config Tools\n",
    "        self.rst = None\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.encoder_dict = dict()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.natural_interval_model = 'kmeans'\n",
    "        self.NIM_message = 'Model for initiat the natural intervals cannot be blank'\n",
    "        self.comb_max_depth = comb_max_depth\n",
    "        self.all_combinations = None\n",
    "        \n",
    "        self.output_loction = os.getcwd() if not output_loction else os.getcwd() + '\\\\' + output_loction\n",
    "        self.save_output = save_output\n",
    "\n",
    "    def fit(\n",
    "            self,\n",
    "            x: pd.DataFrame,\n",
    "            continous_columns: list = [],\n",
    "            natural_interval_model: str = 'kmeans',\n",
    "            ensamble_threshold: int = 0.9\n",
    "    ):\n",
    "        \"\"\"Discretization using Rough Sets Theory (RST).\n",
    "\n",
    "        Fit `x` to an efficient intervals using the concept of RST \n",
    "        * asd\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : DataFrame\n",
    "            Full DataFrame (default is None)\n",
    "\n",
    "        continous_columns : list\n",
    "            Determind names of column(s) in the DataFarme as Continuous data. If `None` a function _check_continuous triger to determind names of column(s).\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        NotMatchResult\n",
    "            If continous_columns `~None` model check the input with the checker determation if didn't match the result\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Reduct Data Intervals phase: Pre-process of discretization, intiate classification\n",
    "        # Pre-process of discretization\n",
    "        self.ensamble_theshold = ensamble_threshold\n",
    "        self.data = x\n",
    "        self.columns = self.data.columns\n",
    "        self.continuous_columns = continous_columns if continous_columns else self._check_continuous()\n",
    "        self.discrete_columns = list(set(self.columns).difference(self.continuous_columns))\n",
    "        self.discrete_columns.remove(self.decision_column_name)\n",
    "        self.scaled_data = self._scaling_continuous()\n",
    "        self.silhouette_scores = self._get_silhouette_score()\n",
    "\n",
    "        # Intiate classification\n",
    "        self.data_after_INI = getattr(self, '_%s_model' % natural_interval_model, lambda: self.NIM_message)()\n",
    "        self.all_combinations = self.get_combinations()\n",
    "        print(self.data_after_INI.nunique())\n",
    "\n",
    "        data = self.data.copy()  # local data vriable to save change in\n",
    "        data = self._labeling_discrete(data)\n",
    "        self.data_after_INI.columns = map((lambda x: x + '_AFTER'), self.data_after_INI.columns)\n",
    "\n",
    "        data = pd.concat([\n",
    "            data.reset_index(drop=True),\n",
    "            self.data_after_INI.reset_index(drop=True)\n",
    "        ], axis=1)\n",
    "\n",
    "        dependency_original_dict = {}\n",
    "        dependency_dict = {}\n",
    "        \n",
    "        # Duplicate Continuous columns\n",
    "        for continuous_att in self.continuous_columns:\n",
    "            att_AFTER = continuous_att + '_AFTER'\n",
    "            label_simulator = self.silhouette_scores[continuous_att]\n",
    "\n",
    "            self.prepared_data = data.copy()\n",
    "            self.prepared_data.sort_values([continuous_att], inplace=True, ignore_index=True)\n",
    "            temp_data = self.prepared_data.copy()\n",
    "            for n_label, o_label in enumerate(temp_data[att_AFTER].unique()):\n",
    "                indices_change_label = temp_data[temp_data[att_AFTER] == o_label].index.tolist()\n",
    "                self.prepared_data.loc[indices_change_label, (att_AFTER)] = n_label\n",
    "            self.prepared_data.sort_values([att_AFTER, continuous_att], inplace=True)\n",
    "\n",
    "            \n",
    "\n",
    "            value = 0\n",
    "            upper_flag = True\n",
    "            lower_flag = True\n",
    "            value_flag = True\n",
    "            value_sufx = ''\n",
    "\n",
    "            while value < label_simulator:\n",
    "                if value_flag:\n",
    "                    upper_flag = True\n",
    "                    lower_flag = True\n",
    "                    value_sufx = ''\n",
    "                indices = self.prepared_data[self.prepared_data[att_AFTER] == value].index.tolist()\n",
    "                if not len(indices) == 0:\n",
    "                    value_dict = '%s%s' % (value, value_sufx)\n",
    "                    if value not in dependency_dict.keys():\n",
    "                        dependency_dict.update({value_dict: {}})\n",
    "                        if upper_flag:\n",
    "                            dependency_dict[value_dict].update({\n",
    "                                'upper': {True: 0, False: 0}\n",
    "                            })\n",
    "\n",
    "                        if lower_flag:\n",
    "                            dependency_dict[value_dict].update({\n",
    "                                'lower': {True: 0, False: 0}\n",
    "                            })\n",
    "\n",
    "                    for combination in self.all_combinations:\n",
    "                        _combination = '|'.join(i for i in combination)\n",
    "                        _combination_AFTER = list(list(combination) + [att_AFTER])\n",
    "\n",
    "                        self.lower_data = self.prepared_data.copy()\n",
    "                        self.upper_data = self.prepared_data.copy()\n",
    "                        \n",
    "                        self.rst = RST(\n",
    "                            self.prepared_data,\n",
    "                            continuous_columns=self.continuous_columns,\n",
    "                            decision_column_name=self.decision_column_name,\n",
    "                            include_continuous_col=True\n",
    "                        )\n",
    "                        _, dependency = self.rst.get_dependency(combination=list(_combination_AFTER))\n",
    "                        dependency_original_dict.update({_combination: dependency})\n",
    "                        \n",
    "                        \n",
    "                        self._set_lower_edge_value(continuous_att, indices, value, label_simulator)\n",
    "                        self._set_upper_edge_value(continuous_att, indices, value, label_simulator)\n",
    "                        \n",
    "                        if value not in [0] and lower_flag:\n",
    "                            self.lower_data.loc[self.lower_edge_value[value]['idx'], (att_AFTER)] = value - 1\n",
    "                            self.rst = RST(\n",
    "                                self.lower_data,\n",
    "                                continuous_columns=self.continuous_columns,\n",
    "                                decision_column_name=self.decision_column_name,\n",
    "                                include_continuous_col=True\n",
    "                            )\n",
    "                            _, dependency = self.rst.get_dependency(\n",
    "                                combination=list(_combination_AFTER)\n",
    "                            )\n",
    "                            dependency_dict[value_dict]['lower'][dependency >= dependency_original_dict[_combination]] += 1\n",
    "                    \n",
    "                        if value not in [label_simulator - 1] and upper_flag:\n",
    "                            self.upper_data.loc[self.upper_edge_value[value]['idx'], (att_AFTER)] = value + 1\n",
    "                            self.rst = RST(\n",
    "                                self.upper_data,\n",
    "                                continuous_columns=self.continuous_columns,\n",
    "                                decision_column_name=self.decision_column_name,\n",
    "                                include_continuous_col=True\n",
    "                            )\n",
    "                            lower, dependency = self.rst.get_dependency(\n",
    "                                combination=list(_combination_AFTER)\n",
    "                            )\n",
    "                            dependency_dict[value_dict]['upper'][dependency >= dependency_original_dict[_combination]] += 1\n",
    "\n",
    "                    if lower_flag:\n",
    "                        is_lower = len(self.all_combinations)*self.ensamble_theshold <= dependency_dict[value_dict]['lower'][True]\n",
    "                    if upper_flag:\n",
    "                        is_upper = len(self.all_combinations)*self.ensamble_theshold <= dependency_dict[value_dict]['upper'][True]\n",
    "\n",
    "                    if is_lower or is_upper:\n",
    "                        lower_flag=is_lower\n",
    "                        upper_flag=is_upper\n",
    "                        value_flag=False\n",
    "                        value_sufx += '-'\n",
    "                        if is_lower:\n",
    "                            self.prepared_data.loc[self.lower_edge_value[value]['idx'], (att_AFTER)] = value - 1\n",
    "                            value_sufx += 'l'\n",
    "                        if is_upper:\n",
    "                            self.prepared_data.loc[self.upper_edge_value[value]['idx'], (att_AFTER)] = value + 1\n",
    "                            value_sufx += 'u'\n",
    "                    else:\n",
    "                        value += 1\n",
    "                        value_flag = True\n",
    "                else:\n",
    "                    value += 1\n",
    "                    value_flag = True\n",
    "\n",
    "        dependency_dict = collections.OrderedDict(sorted(dependency_dict.items()))\n",
    "        if self.save_output:\n",
    "            Path(self.output_loction + '\\\\_log').mkdir(parents=True, exist_ok=True)\n",
    "            with open(self.output_loction + '\\\\_log\\\\logMap.json', 'w+') as fp:\n",
    "                json.dump(dependency_dict, fp)\n",
    "                \n",
    "            self.prepared_data.to_csv(self.output_loction + '\\\\DataAfterDRST.csv')\n",
    "            data.to_csv(self.output_loction + '\\\\DataBeforeDRST.csv')\n",
    "        return [self.prepared_data, self.continuous_columns]\n",
    "\n",
    "    def _dbscan_model(self):\n",
    "        result = {}\n",
    "        for att in self.continuous_columns:\n",
    "            clustering = DBSCAN(eps=1, min_samples=5).fit(\n",
    "                self._np_array_reshaped(self.data[att])\n",
    "            )\n",
    "            result[att] = clustering.labels_\n",
    "        return pd.DataFrame(result)\n",
    "\n",
    "    def _kmeans_model(self):\n",
    "        result = {}\n",
    "        for att in self.continuous_columns:\n",
    "            clustering = KMeans(n_clusters=self.silhouette_scores[att], random_state=41).fit(\n",
    "                self._np_array_reshaped(self.scaled_data[att])\n",
    "            )\n",
    "            result[att] = clustering.labels_\n",
    "        return pd.DataFrame(result)\n",
    "\n",
    "    def _np_array_reshaped(self, data, reshape=(-1, 1)):\n",
    "        return np.array(data.tolist()).reshape(reshape[0], reshape[1])\n",
    "\n",
    "    def _check_continuous(self, continous_columns: list = []):\n",
    "        top_n = self.checker_top_n\n",
    "        likely = []\n",
    "\n",
    "        # check if attribute is continuous or discrete dataframe\n",
    "        for var in self.data.columns:\n",
    "            if self.dis_checker == 'topN':\n",
    "                # Check if the top n unique values account for more than a certain proportion of all values\n",
    "                if 1.*self.data[var].value_counts(normalize=True).head(top_n).sum() < 0.5:\n",
    "                    likely.append(var)\n",
    "            elif self.dis_checker == 'ratio':\n",
    "                # Find the ratio of number of unique values to the total number of unique values. Something like the following\n",
    "                if 1.*self.data[var].nunique()/self.data[var].count() > 0.5:\n",
    "                    likely.append(var)\n",
    "\n",
    "        common_names = ['id']\n",
    "\n",
    "        return [x for x in likely if x.lower() not in common_names]\n",
    "\n",
    "    def _labeling_discrete(self, data):\n",
    "\n",
    "        data_endocded = data.copy()\n",
    "\n",
    "        for col in data_endocded:\n",
    "            if col not in self.continuous_columns:\n",
    "                le = self.encoder.fit(data_endocded[col])\n",
    "                data_endocded[col] = self.encoder.transform(data_endocded[col])\n",
    "                self.encoder_dict[col] = self.encoder\n",
    "        return data_endocded\n",
    "\n",
    "    def _scaling_continuous(self):\n",
    "        print(self.continuous_columns)\n",
    "        data_cluster = self.data[self.continuous_columns].copy()\n",
    "        scaled_columns = self.scaler.fit_transform(data_cluster)\n",
    "        self.scaled_data = self.data.copy()\n",
    "        self.scaled_data[self.continuous_columns] = scaled_columns\n",
    "\n",
    "        return self.scaled_data\n",
    "\n",
    "    def _get_silhouette_score(self):\n",
    "        scores = {}\n",
    "        for att in self.continuous_columns:\n",
    "            temp_score = []\n",
    "            scores[att] = 3\n",
    "            for n_clusters in range(3, 9):\n",
    "                # Initialize the clusterer with n_clusters value and a random generator\n",
    "                # seed of 10 for reproducibility.\n",
    "                clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "                cluster_labels = clusterer.fit_predict(\n",
    "                    self._np_array_reshaped(self.scaled_data[att]))\n",
    "\n",
    "                # The silhouette_score gives the average value for all the samples.\n",
    "                # This gives a perspective into the density and separation of the formed\n",
    "                # clusters\n",
    "                silhouette_avg = silhouette_score(\n",
    "                    self._np_array_reshaped(self.scaled_data[att]), cluster_labels)\n",
    "                temp_score.append(silhouette_avg)\n",
    "            scores[att] = temp_score.index(max(temp_score)) + 3\n",
    "        return scores\n",
    "\n",
    "    def get_combinations(self, comb_max_depth: int = None):\n",
    "        if not comb_max_depth:\n",
    "            comb_max_depth = self.comb_max_depth\n",
    "        combX = []\n",
    "        for idx, _ in enumerate(self.discrete_columns):\n",
    "            if comb_max_depth and comb_max_depth == idx:\n",
    "                break\n",
    "            combX.extend(list(combinations(self.discrete_columns, idx + 1)))\n",
    "\n",
    "        return sorted(combX, key=len)\n",
    "    \n",
    "    def _set_lower_edge_value(self, att, indices, value, s_c):\n",
    "        same_value = self.prepared_data.loc[indices][att].values.tolist()\n",
    "        lower_same_value = self.prepared_data[att].isin([same_value[0]])\n",
    "        lower_temp = self.prepared_data[lower_same_value]\n",
    "        self.lower_edge_value.update({\n",
    "            value: {\n",
    "                'idx': lower_temp.index.tolist(),\n",
    "                'value': lower_temp.values.tolist()[0][0],\n",
    "            }\n",
    "        })\n",
    "\n",
    "        return self.lower_edge_value\n",
    "\n",
    "    def _set_upper_edge_value(self, att, indices, value, s_c):\n",
    "        same_value = self.prepared_data.loc[indices][att].values.tolist()\n",
    "        upper_same_value = self.prepared_data[att].isin([same_value[-1]])\n",
    "        upper_temp = self.prepared_data[upper_same_value]\n",
    "        self.upper_edge_value.update({\n",
    "            value: {\n",
    "                'idx': upper_temp.index.tolist(),\n",
    "                'value': upper_temp.values.tolist()[0][0],\n",
    "            }\n",
    "        })\n",
    "\n",
    "        return self.upper_edge_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      int64\n",
       "1     object\n",
       "2     object\n",
       "3      int64\n",
       "4     object\n",
       "5     object\n",
       "6     object\n",
       "7     object\n",
       "8     object\n",
       "9     object\n",
       "10    object\n",
       "11    object\n",
       "12    object\n",
       "13    object\n",
       "14    object\n",
       "15     int64\n",
       "16     int64\n",
       "17    object\n",
       "18    object\n",
       "19    object\n",
       "20    object\n",
       "21    object\n",
       "22    object\n",
       "23    object\n",
       "24    object\n",
       "25    object\n",
       "26    object\n",
       "27    object\n",
       "28    object\n",
       "29    object\n",
       "30    object\n",
       "31    object\n",
       "32    object\n",
       "33    object\n",
       "34    object\n",
       "35    object\n",
       "36    object\n",
       "37    object\n",
       "38    object\n",
       "39    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('datasets/bands.csv', header=None)\n",
    "data = data.replace('?',np.NaN)\n",
    "data.columns = [str(c) for c in data.columns]\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '3', '30']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'X126'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\Thesis\\An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory\\test.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Thesis/An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory/test.ipynb#ch0000001?line=0'>1</a>\u001b[0m drst \u001b[39m=\u001b[39m DRST(comb_max_depth\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, decision_column_name\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mcolumns[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], save_output\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Thesis/An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory/test.ipynb#ch0000001?line=1'>2</a>\u001b[0m drst_fit, continuous_columns \u001b[39m=\u001b[39m drst\u001b[39m.\u001b[39;49mfit(data, ensamble_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Thesis/An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory/test.ipynb#ch0000001?line=2'>3</a>\u001b[0m \u001b[39m# drst_fit.drop(continuous_columns, axis=1, inplace=True)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Thesis/An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory/test.ipynb#ch0000001?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(continuous_columns)\n",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\Thesis\\An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory\\test.ipynb Cell 3\u001b[0m in \u001b[0;36mDRST.fit\u001b[1;34m(self, x, continous_columns, natural_interval_model, ensamble_threshold)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Thesis/An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory/test.ipynb#ch0000001?line=130'>131</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscrete_columns \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\u001b[39m.\u001b[39mdifference(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinuous_columns))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Thesis/An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory/test.ipynb#ch0000001?line=131'>132</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscrete_columns\u001b[39m.\u001b[39mremove(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_column_name)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Thesis/An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory/test.ipynb#ch0000001?line=132'>133</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaled_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scaling_continuous()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Thesis/An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory/test.ipynb#ch0000001?line=133'>134</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msilhouette_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_silhouette_score()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Thesis/An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory/test.ipynb#ch0000001?line=135'>136</a>\u001b[0m \u001b[39m# Intiate classification\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\Thesis\\An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory\\test.ipynb Cell 3\u001b[0m in \u001b[0;36mDRST._scaling_continuous\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Thesis/An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory/test.ipynb#ch0000001?line=324'>325</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinuous_columns)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Thesis/An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory/test.ipynb#ch0000001?line=325'>326</a>\u001b[0m data_cluster \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinuous_columns]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Thesis/An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory/test.ipynb#ch0000001?line=326'>327</a>\u001b[0m scaled_columns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mfit_transform(data_cluster)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Thesis/An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory/test.ipynb#ch0000001?line=327'>328</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaled_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Thesis/An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory/test.ipynb#ch0000001?line=328'>329</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaled_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinuous_columns] \u001b[39m=\u001b[39m scaled_columns\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Thesis\\An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory\\venv\\lib\\site-packages\\sklearn\\base.py:852\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    850\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 852\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    853\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    854\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Thesis\\An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:806\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 806\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Thesis\\An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:841\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[39m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \n\u001b[0;32m    811\u001b[0m \u001b[39mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[39m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    839\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    840\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 841\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    842\u001b[0m     X,\n\u001b[0;32m    843\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    844\u001b[0m     estimator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    845\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    846\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    847\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[0;32m    848\u001b[0m )\n\u001b[0;32m    849\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    851\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Thesis\\An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory\\venv\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    565\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 566\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    567\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    568\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Thesis\\An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Thesis\\An-Efficient-Discretization-Algorithm-Using-Rough-Set-Theory\\venv\\lib\\site-packages\\pandas\\core\\generic.py:2072\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2072\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'X126'"
     ]
    }
   ],
   "source": [
    "drst = DRST(comb_max_depth=1, decision_column_name=data.columns[-1], save_output=False)\n",
    "drst_fit, continuous_columns = drst.fit(data, ensamble_threshold=0.9)\n",
    "# drst_fit.drop(continuous_columns, axis=1, inplace=True)\n",
    "print(continuous_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual_Premium    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('datasets/health_insurance.csv')\n",
    "df = data.copy()\n",
    "df = df.sample(2000, random_state=41) # 41\n",
    "df.drop(['Vintage', 'Age'], axis=1, inplace=True)\n",
    "drst = DRST(comb_max_depth=1, decision_column_name='Response', save_output=False)\n",
    "drst_fit, continuous_columns = drst.fit(df, continous_columns=['Annual_Premium'])\n",
    "drst_fit.drop(continuous_columns, axis=1, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d79206066a55b881ba84cc6d6921fa129f4f3f88d457a15c3c93c97d3ef4d533"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
